# ğŸ“Š Phonofix è®Šé«”ç”Ÿæˆå™¨æ·±åº¦åˆ†æå ±å‘Š

## âœ… æ ¸å¿ƒç†å¿µé©—è­‰

ä½ çš„ç†å¿µéå¸¸æ¸…æ™°ä¸”æ­£ç¢ºï¼š**ã€Œåœ¨æ‹¼éŸ³ç¶­åº¦é€²è¡Œæ¯”å°ï¼Œè®Šé«”çš„æ‹¼å¯«åªæ˜¯ UX å±•ç¤ºã€**

é€šéä»£ç¢¼è¿½è¹¤ï¼Œæˆ‘ç¢ºèªäº†ï¼š
- âœ… Chinese: è®Šé«”å»é‡åŸºæ–¼ Pinyin (`_filter_aliases_by_pinyin`)
- âœ… Japanese: è®Šé«”å»é‡åŸºæ–¼ Romaji (`filter_homophones`)
- âš ï¸ **English: è¨­è¨ˆèˆ‡ç†å¿µä¸ä¸€è‡´**ï¼ˆè©³è¦‹ä¸‹æ–¹ï¼‰

---

## ğŸ”´ é—œéµå•é¡Œï¼šæŒ‰å„ªå…ˆç´šæ’åº

### P0 - å¿…é ˆä¿®å¾©

#### 1ï¸âƒ£ **English æ¨¡çµ„æ¶æ§‹å•é¡Œ** â­â­â­â­â­
**å•é¡Œ**ï¼šå®Œå…¨æ²’æœ‰åœ¨ IPA éŸ³æ¨™ç¶­åº¦ç”Ÿæˆè®Šé«”
- ç•¶å‰å¯¦ç¾ï¼šåŸºæ–¼æ‹¼å¯«è¦å‰‡çš„ç¡¬ç·¨ç¢¼æ¨¡å¼åŒ¹é…
- ç†æƒ³å¯¦ç¾ï¼šterm â†’ IPA â†’ æ¨¡ç³Š IPA è®Šé«” â†’ åæ¨æ‹¼å¯«
- **å½±éŸ¿**ï¼šç„¡æ³•æ³›åŒ–åˆ°æ–°è©ï¼Œèˆ‡ä¸­æ–‡/æ—¥æ–‡è¨­è¨ˆç†å¿µä¸ä¸€è‡´

**è­‰æ“š**ï¼š
```python
# fuzzy_generator.py å®Œå…¨æ²’æœ‰èª¿ç”¨ phonetic_impl.py
# åªæœ‰ç¡¬ç·¨ç¢¼çš„ ASR_SPLIT_PATTERNS å’Œ SPELLING_PATTERNS
```

**å»ºè­°æ–¹æ¡ˆ**ï¼š
```python
class EnglishFuzzyGenerator:
    def generate_variants(self, term: str) -> List[str]:
        # 1. term â†’ IPA
        ipa = self.phonetic.to_phonetic(term)

        # 2. ç”Ÿæˆ IPA æ¨¡ç³Šè®Šé«” (åŸºæ–¼éŸ³ç´ ç›¸ä¼¼åº¦è¦å‰‡)
        ipa_variants = self._generate_ipa_fuzzy_variants(ipa)

        # 3. IPA â†’ æ‹¼å¯« (å¯ç”¨ CMU Pronouncing Dictionary)
        spelling_variants = self._ipa_to_spellings(ipa_variants)

        # 4. è£œå……ç¡¬ç·¨ç¢¼è¦å‰‡ (ä½œç‚ºå¾Œå‚™)
        spelling_variants.extend(self._apply_asr_patterns(term))

        return self._deduplicate_by_ipa(spelling_variants)
```

**å¯¦ç¾é›£é»**ï¼š
- IPA â†’ æ‹¼å¯«çš„åå‘æ˜ å°„å›°é›£ (ä¸€å°å¤š)
- å»ºè­°ï¼šçµåˆ CMU Dict + ä¿ç•™ç¾æœ‰ ASR_SPLIT_PATTERNS ä½œç‚ºè£œå……

---

#### 2ï¸âƒ£ **Chinese æ¨¡çµ„æ€§èƒ½å•é¡Œ** â­â­â­â­
**å•é¡Œ**ï¼šç¬›å¡çˆ¾ç©çˆ†ç‚¸ + å¾Œç½®å»é‡æ•ˆç‡ä½

**ç•¶å‰æµç¨‹**ï¼š
```python
# 1. ç”Ÿæˆæ‰€æœ‰å­—çš„çµ„åˆ (å¯èƒ½ 625+ çµ„åˆ)
for combo in itertools.product(*char_options_list):
    word = "".join([item["char"] for item in combo])
    combinations.append(word)

# 2. å¾ŒçºŒåœ¨ corrector.py ä¸­å»é‡
filtered = _filter_aliases_by_pinyin(aliases, utils)
```

**å•é¡Œåˆ†æ**ï¼š
- 4å­—è© Ã— 5è®Šé«”/å­— = 625 å€‹çµ„åˆ
- å…¶ä¸­å¾ˆå¤šæ‹¼éŸ³é‡è¤‡ï¼ˆä¸åŒå­—ä½†åŒéŸ³ï¼‰
- åœ¨ `corrector.py` æ‰å»é‡ï¼Œæµªè²»è¨ˆç®—

**å„ªåŒ–æ–¹æ¡ˆ**ï¼š
```python
def _generate_char_combinations(self, char_options_list):
    """åœ¨ç”Ÿæˆéšæ®µå°±åŸºæ–¼æ‹¼éŸ³å»é‡"""
    seen_pinyins = set()
    combinations = []

    # é™åˆ¶çµ„åˆæ•¸
    MAX_COMBOS = 200

    for i, combo in enumerate(itertools.product(*char_options_list)):
        if i >= MAX_COMBOS:
            break

        # æå‰è¨ˆç®—æ‹¼éŸ³ä¸¦å»é‡
        pinyin = "".join([item["pinyin"] for item in combo])
        if pinyin in seen_pinyins:
            continue

        word = "".join([item["char"] for item in combo])
        combinations.append(word)
        seen_pinyins.add(pinyin)

    return combinations
```

**é æœŸæ•ˆæœ**ï¼š
- æ¸›å°‘ 60-80% ç„¡æ•ˆçµ„åˆ
- é™ä½å¾ŒçºŒè™•ç†è² æ“”

---

#### 3ï¸âƒ£ **Japanese æ¨¡çµ„çš„ä»»æ„é™åˆ¶** â­â­â­
**å•é¡Œ**ï¼šç¡¬ç·¨ç¢¼çš„æ•¸å­—é™åˆ¶æ²’æœ‰ä¾æ“š

```python
# Line 189-191: ç‚ºä»€éº¼æ˜¯ 50ï¼Ÿ
for i, combo in enumerate(itertools.product(*char_options)):
    if i > 50:
        break

# Line 201: ç‚ºä»€éº¼æ˜¯ 10ï¼Ÿ
for k_var in list(final_kana_variants)[:10]:
```

**å»ºè­°**ï¼šåŸºæ–¼è©é•·å‹•æ…‹èª¿æ•´
```python
def generate_variants(self, term: str, max_variants: int = 30):
    hira_parts = self._kanji_to_hiragana_list(term)
    base_hira = "".join(hira_parts)

    # å‹•æ…‹è¨ˆç®—ä¸Šé™
    max_kana_combos = min(200, 10 ** min(len(base_hira), 3))
    max_romaji_sources = min(20, len(base_hira) * 2)

    # ä½¿ç”¨å‹•æ…‹ä¸Šé™
    for i, combo in enumerate(itertools.product(*char_options)):
        if i >= max_kana_combos:
            break

    for k_var in list(final_kana_variants)[:max_romaji_sources]:
        # è½‰ç¾…é¦¬å­—
```

---

### P1 - é‡è¦æ”¹é€²

#### 4ï¸âƒ£ **çµ±ä¸€æ¶æ§‹ç¼ºå¤±** â­â­â­â­
**å•é¡Œ**ï¼šä¸‰å€‹èªè¨€æ¨¡çµ„æ²’æœ‰çµ±ä¸€æŠ½è±¡

**å»ºè­°**ï¼šæŠ½è±¡ `BaseFuzzyGenerator`
```python
# src/phonofix/core/fuzzy_generator_interface.py
from abc import ABC, abstractmethod

class BaseFuzzyGenerator(ABC):
    """è®Šé«”ç”Ÿæˆå™¨æŠ½è±¡åŸºé¡"""

    @abstractmethod
    def phonetic_transform(self, term: str) -> str:
        """æ–‡å­— â†’ éŸ³æ¨™ (Pinyin/IPA/Romaji)"""
        pass

    @abstractmethod
    def generate_phonetic_variants(self, phonetic: str) -> List[str]:
        """éŸ³æ¨™ â†’ æ¨¡ç³ŠéŸ³æ¨™è®Šé«”"""
        pass

    @abstractmethod
    def phonetic_to_representative_text(self, phonetic: str) -> str:
        """éŸ³æ¨™ â†’ ä»£è¡¨æ–‡å­— (UXå±•ç¤º)"""
        pass

    def generate_variants(self, term: str) -> List[str]:
        """çµ±ä¸€æµç¨‹"""
        # 1. è½‰éŸ³æ¨™
        phonetic = self.phonetic_transform(term)

        # 2. ç”ŸæˆéŸ³æ¨™è®Šé«”
        phonetic_variants = self.generate_phonetic_variants(phonetic)

        # 3. éŸ³æ¨™ â†’ ä»£è¡¨æ–‡å­—
        text_variants = []
        for p_var in phonetic_variants:
            text = self.phonetic_to_representative_text(p_var)
            text_variants.append(text)

        # 4. åŸºæ–¼éŸ³æ¨™å»é‡
        return self.deduplicate_by_phonetic(text_variants)
```

**å„ªé»**ï¼š
- å¼·åˆ¶ä¸‰å€‹èªè¨€ä½¿ç”¨ç›¸åŒæ¶æ§‹
- å®¹æ˜“æ·»åŠ æ–°èªè¨€ (éŸ“æ–‡ã€æ³°æ–‡)
- English æ¨¡çµ„è¢«è¿«æ­£ç¢ºå¯¦ç¾

---

#### 5ï¸âƒ£ **è®Šé«”è³ªé‡è©•åˆ†ç¼ºå¤±** â­â­â­
**å•é¡Œ**ï¼šæ‰€æœ‰è®Šé«”å¹³ç­‰å°å¾…ï¼Œæ²’æœ‰é‡è¦æ€§å€åˆ†

**å»ºè­°**ï¼šæ·»åŠ ç½®ä¿¡åº¦è©•åˆ†
```python
@dataclass
class FuzzyVariant:
    text: str           # è®Šé«”æ‹¼å¯«
    phonetic: str       # éŸ³æ¨™
    score: float        # ç½®ä¿¡åº¦ (0.0-1.0)
    source: str         # ä¾†æº: "initial"|"final"|"special"|"sticky"

def generate_scored_variants(self, term: str) -> List[FuzzyVariant]:
    """ç”Ÿæˆå¸¶è©•åˆ†çš„è®Šé«”"""
    variants = []

    # è²æ¯è®Šé«” (å¸¸è¦‹) - é«˜åˆ†
    for v in self._generate_initial_variants(term):
        variants.append(FuzzyVariant(v, ..., score=0.8, source="initial"))

    # éŸ»æ¯è®Šé«” (è¼ƒå¸¸è¦‹) - ä¸­é«˜åˆ†
    for v in self._generate_final_variants(term):
        variants.append(FuzzyVariant(v, ..., score=0.6, source="final"))

    # ç‰¹æ®ŠéŸ³ç¯€ (å°‘è¦‹) - ä¸­åˆ†
    for v in self._generate_special_variants(term):
        variants.append(FuzzyVariant(v, ..., score=0.4, source="special"))

    # æŒ‰åˆ†æ•¸æ’åº
    return sorted(variants, key=lambda x: x.score, reverse=True)
```

**ç”¨é€”**ï¼š
- ä½¿ç”¨è€…å¯è¨­å®šæœ€å°ç½®ä¿¡åº¦é–¾å€¼
- é™åˆ¶è®Šé«”æ•¸é‡æ™‚å„ªå…ˆä¿ç•™é«˜åˆ†è®Šé«”
- æ¯”å°æ™‚é«˜åˆ†è®Šé«”æ¬Šé‡æ›´é«˜

---

#### 6ï¸âƒ£ **Config å¯æ“´å±•æ€§ä¸è¶³** â­â­â­
**å•é¡Œ**ï¼šç¡¬ç·¨ç¢¼è¦å‰‡ç„¡æ³•å‹•æ…‹æ“´å±•

**English Config å•é¡Œæœ€åš´é‡**ï¼š
```python
# 140+ è¡Œç¡¬ç·¨ç¢¼å­—å…¸
ASR_SPLIT_PATTERNS = {
    'tensorflow': [...],
    'pytorch': [...],
    # æ–°æŠ€è¡“è©å½™å¦‚ "Ollama", "LangChain" ç„¡æ³•è™•ç†
}
```

**å»ºè­°æ–¹æ¡ˆ**ï¼š
```python
class EnglishPhoneticConfig:
    # å…§å»ºè¦å‰‡
    _DEFAULT_ASR_PATTERNS = {...}

    def __init__(self):
        self.asr_patterns = self._DEFAULT_ASR_PATTERNS.copy()

    def register_asr_pattern(self, term: str, variants: List[str]):
        """å…è¨±ä½¿ç”¨è€…å‹•æ…‹æ·»åŠ è¦å‰‡"""
        self.asr_patterns[term.lower()] = variants

    def load_patterns_from_file(self, path: str):
        """å¾ YAML/JSON è¼‰å…¥è¦å‰‡"""
        import yaml
        with open(path) as f:
            custom = yaml.safe_load(f)
        self.asr_patterns.update(custom)

# ä½¿ç”¨
config = EnglishPhoneticConfig()
config.register_asr_pattern("ollama", ["oh llama", "o lama"])
engine = EnglishEngine(config=config)
```

---

### P2 - å„ªåŒ–å»ºè­°

#### 7ï¸âƒ£ **é‚Šç•Œæ¢ä»¶è™•ç†ä¸å®Œæ•´** â­â­

**Chinese æ··åˆæ–‡æœ¬å•é¡Œ**ï¼š
```python
# ç•¶å‰: "PyTorchæ¨¡å‹" â†’ "P", "y", "T", ... éƒ½è¿”å›åŸæ¨£
# _get_char_variations() Line 137
if not ('\u4e00' <= char <= '\u9fff'):
    return [{"pinyin": char, "char": char}]

# å•é¡Œ: è‹±æ–‡å­—æ¯æ²’æœ‰è®Šé«”ï¼Œä½†å¯¦éš›å¯èƒ½æœ‰æ‹¼å¯«éŒ¯èª¤
```

**å»ºè­°**ï¼š
```python
def _get_char_variations(self, char):
    if '\u4e00' <= char <= '\u9fff':
        # ä¸­æ–‡å­—ç¬¦è™•ç†
        return self._get_chinese_variations(char)
    elif char.isalpha():
        # è‹±æ–‡å­—ç¬¦ï¼šè¿”å›å¸¸è¦‹æ‹¼å¯«éŒ¯èª¤
        return [{"pinyin": char, "char": char},
                {"pinyin": char.lower(), "char": char.lower()},
                {"pinyin": char.upper(), "char": char.upper()}]
    else:
        # å…¶ä»–å­—ç¬¦
        return [{"pinyin": char, "char": char}]
```

---

#### 8ï¸âƒ£ **éŒ¯èª¤è™•ç†ä¸å¤ å‹å¥½** â­â­

**Japanese æ¨¡çµ„**ï¼š
```python
# Line 202-209: try-except å¤ªå¯¬æ³›
try:
    r_base = cutlet_katsu.romaji(k_var)
    # ...
except Exception:  # åƒæ‰æ‰€æœ‰éŒ¯èª¤ï¼
    continue
```

**å»ºè­°**ï¼š
```python
try:
    r_base = cutlet_katsu.romaji(k_var)
except Exception as e:
    logger.warning(f"Romaji conversion failed for '{k_var}': {e}")
    continue
```

---

#### 9ï¸âƒ£ **å€™é¸å­—æ•¸é™åˆ¶éæ–¼ä¿å®ˆ** â­â­

**Chinese fuzzy_generator.py**ï¼š
```python
# Line 86: max_chars=2 å¤ªå°‘
def _pinyin_to_chars(self, pinyin_str, max_chars=2):
```

**å•é¡Œ**ï¼šæŸäº›æ‹¼éŸ³æœ‰ 5+ å€‹å¸¸ç”¨åŒéŸ³å­—ï¼ˆå¦‚ "yi"ï¼‰

**å»ºè­°**ï¼š
```python
def _pinyin_to_chars(self, pinyin_str, max_chars=None):
    """å‹•æ…‹èª¿æ•´å€™é¸æ•¸"""
    if max_chars is None:
        # çŸ­éŸ³ç¯€å–æ›´å¤šå€™é¸ (å¸¸ç”¨éŸ³ç¯€åŒéŸ³å­—å¤š)
        max_chars = 6 if len(pinyin_str) <= 3 else 3

    # ... DAG æŸ¥è©¢
```

---

## ğŸ¯ å…·é«”ä»£ç¢¼æ”¹é€²ç¯„ä¾‹

### Chinese: åœ¨ç”Ÿæˆéšæ®µå»é‡

```python
# src/phonofix/languages/chinese/fuzzy_generator.py
def _generate_char_combinations(self, char_options_list):
    """ç”Ÿæˆå­—ç¬¦çµ„åˆ (å„ªåŒ–ç‰ˆï¼šæå‰åŸºæ–¼æ‹¼éŸ³å»é‡)"""
    seen_pinyins = set()
    combinations = []

    # æ ¹æ“šè©é•·å‹•æ…‹è¨­å®šä¸Šé™
    word_len = len(char_options_list)
    MAX_COMBOS = min(300, 100 * word_len)

    for i, combo in enumerate(itertools.product(*char_options_list)):
        if i >= MAX_COMBOS:
            logger.warning(f"é”åˆ°çµ„åˆä¸Šé™ {MAX_COMBOS}ï¼Œæˆªæ–·è®Šé«”ç”Ÿæˆ")
            break

        # è¨ˆç®—æ‹¼éŸ³ä¸¦æå‰å»é‡
        pinyin = "".join([item["pinyin"] for item in combo])
        if pinyin in seen_pinyins:
            continue

        word = "".join([item["char"] for item in combo])
        combinations.append(word)
        seen_pinyins.add(pinyin)

    return combinations
```

---

### English: IPA ç¶­åº¦é‡æ§‹

```python
# src/phonofix/languages/english/fuzzy_generator.py
class EnglishFuzzyGenerator:
    def __init__(self, config=None):
        self.config = config or EnglishPhoneticConfig()
        self.phonetic = EnglishPhoneticSystem()

        # è¼‰å…¥ CMU Pronouncing Dictionary (å¯é¸)
        self.cmu_dict = self._load_cmu_dict()

    def generate_variants(self, term: str, max_variants: int = 30) -> List[str]:
        """åŸºæ–¼ IPA çš„è®Šé«”ç”Ÿæˆ"""
        variants = set()

        # æ–¹æ³• 1: IPA ç¶­åº¦ç”Ÿæˆ (ä¸»è¦æ–¹æ³•)
        ipa_variants = self._generate_ipa_based_variants(term)
        variants.update(ipa_variants)

        # æ–¹æ³• 2: ç¡¬ç·¨ç¢¼è¦å‰‡ (è£œå……æ–¹æ³•)
        pattern_variants = self._generate_pattern_based_variants(term)
        variants.update(pattern_variants)

        # åŸºæ–¼ IPA å»é‡
        return self._deduplicate_by_ipa(list(variants))[:max_variants]

    def _generate_ipa_based_variants(self, term: str) -> Set[str]:
        """IPA ç¶­åº¦è®Šé«”ç”Ÿæˆ (æ–°å¢)"""
        # 1. term â†’ IPA
        ipa = self.phonetic.to_phonetic(term)

        # 2. ç”Ÿæˆ IPA è®Šé«”
        ipa_variants = self._apply_ipa_fuzzy_rules(ipa)

        # 3. IPA â†’ æ‹¼å¯«å€™é¸
        spelling_variants = set()
        for ipa_var in ipa_variants:
            spellings = self._ipa_to_spellings(ipa_var, term)
            spelling_variants.update(spellings)

        return spelling_variants

    def _apply_ipa_fuzzy_rules(self, ipa: str) -> List[str]:
        """æ‡‰ç”¨ IPA éŸ³ç´ æ¨¡ç³Šè¦å‰‡"""
        variants = {ipa}

        # éŸ³ç´ æ›¿æ›è¦å‰‡ (é¡ä¼¼ Chinese çš„è²æ¯/éŸ»æ¯è¦å‰‡)
        IPA_FUZZY_RULES = {
            # æ¸…æ¿éŸ³æ··æ·†
            ('p', 'b'), ('t', 'd'), ('k', 'É¡'),
            # é•·çŸ­å…ƒéŸ³
            ('iË', 'Éª'), ('uË', 'ÊŠ'),
            # å¸¸è¦‹æ··æ·†
            ('Î¸', 'f'), ('Ã°', 'v'),  # think -> fink
        }

        for rule in IPA_FUZZY_RULES:
            for sound1, sound2 in [rule, rule[::-1]]:
                if sound1 in ipa:
                    variants.add(ipa.replace(sound1, sound2))

        return list(variants)

    def _ipa_to_spellings(self, ipa: str, original: str) -> List[str]:
        """IPA â†’ å¯èƒ½æ‹¼å¯« (ä½¿ç”¨ CMU Dict + è¦å‰‡æ¨æ¸¬)"""
        candidates = []

        # 1. CMU Dict åæŸ¥
        if self.cmu_dict:
            candidates.extend(self.cmu_dict.get(ipa, []))

        # 2. åŸºæ–¼åŸè©çš„éŸ³ç´ ç·¨è¼¯
        candidates.append(original.lower())

        # 3. å¸¸è¦‹éŸ³ç´ â†’å­—æ¯æ˜ å°„
        # (ç°¡åŒ–ç¤ºä¾‹ï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„ G2P åå‘)
        spelling = ipa.replace('Ã°', 'th').replace('Î¸', 'th')
        candidates.append(spelling)

        return candidates
```

---

### Japanese: å‹•æ…‹é™åˆ¶

```python
# src/phonofix/languages/japanese/fuzzy_generator.py
def generate_variants(self, term: str, max_variants: int = 30) -> List[str]:
    """ç”Ÿæˆæ—¥æ–‡è®Šé«” (å„ªåŒ–ç‰ˆï¼šå‹•æ…‹é™åˆ¶)"""
    hira_parts = self._kanji_to_hiragana_list(term)
    base_hira = "".join(hira_parts)

    # å‹•æ…‹è¨ˆç®—ä¸Šé™
    word_len = len(base_hira)
    max_kana_combos = min(200, 10 ** min(word_len, 3))
    max_romaji_sources = min(30, word_len * 3)

    char_options = [self._get_kana_variations(ch) for ch in base_hira]

    kana_combinations = []
    for i, combo in enumerate(itertools.product(*char_options)):
        if i >= max_kana_combos:
            logger.debug(f"é”åˆ°å‡åçµ„åˆä¸Šé™ {max_kana_combos}")
            break
        kana_combinations.append("".join(combo))

    # æ‡‰ç”¨æ•´è©è¦å‰‡
    final_kana_variants = set()
    for combo in kana_combinations:
        final_kana_variants.update(self._apply_kana_phrase_rules(combo))

    # è½‰ç¾…é¦¬å­— (å‹•æ…‹é™åˆ¶)
    cutlet_katsu = _get_cutlet()
    romaji_variants = set()

    for k_var in sorted(final_kana_variants)[:max_romaji_sources]:
        try:
            r_base = cutlet_katsu.romaji(k_var)
            if r_base:
                r_clean = r_base.replace(" ", "")
                romaji_variants.update(self._apply_romaji_config_rules(r_clean))
        except Exception as e:
            logger.warning(f"ç¾…é¦¬å­—è½‰æ›å¤±æ•— '{k_var}': {e}")
            continue

    all_variants = final_kana_variants.union(romaji_variants)
    all_variants.discard(term)

    # åŒéŸ³å»é‡
    variant_list = sorted(list(all_variants), key=lambda x: (len(x), x))
    filtered = self.filter_homophones(variant_list)

    return filtered["kept"][:max_variants]
```

---

## ğŸ“‹ å¯¦æ–½å„ªå…ˆç´šå»ºè­°

å¦‚æœä½ æ™‚é–“æœ‰é™ï¼Œå»ºè­°æŒ‰æ­¤é †åºï¼š

### Phase 1: ä¿®å¾©é—œéµå•é¡Œ (1-2 é€±)
1. **English æ¨¡çµ„é‡æ§‹** - å¯¦ç¾ IPA ç¶­åº¦ç”Ÿæˆ
2. **Chinese æ€§èƒ½å„ªåŒ–** - åœ¨ç”Ÿæˆéšæ®µå»é‡
3. **Japanese å‹•æ…‹é™åˆ¶** - ç§»é™¤ç¡¬ç·¨ç¢¼æ•¸å­—

### Phase 2: æ¶æ§‹æ”¹é€² (1 é€±)
4. **çµ±ä¸€æŠ½è±¡** - å¯¦ç¾ BaseFuzzyGenerator
5. **Config å¯æ“´å±•** - æä¾›å‹•æ…‹æ·»åŠ è¦å‰‡ API

### Phase 3: è³ªé‡æå‡ (1 é€±)
6. **è®Šé«”è©•åˆ†** - æ·»åŠ ç½®ä¿¡åº¦æ©Ÿåˆ¶
7. **é‚Šç•Œæ¢ä»¶** - è™•ç†æ··åˆæ–‡æœ¬ã€éŒ¯èª¤è™•ç†

---

## ğŸ’¡ é¡å¤–å»ºè­°

### æ¸¬è©¦æ•¸æ“šæ”¶é›†
å»ºè­°æ”¶é›†çœŸå¯¦ ASR/LLM éŒ¯èª¤æ•¸æ“šï¼š
```python
# ä½¿ç”¨ Azure Speech/Google STT æ¸¬è©¦ä½ çš„å°ˆæœ‰åè©
test_terms = ["å°åŒ—è»Šç«™", "TensorFlow", "ã‚¢ã‚¹ãƒ”ãƒªãƒ³"]
asr_outputs = [run_asr_test(term) for term in test_terms]

# å°æ¯”ä½ çš„è®Šé«”æ˜¯å¦æ¶µè“‹äº†çœŸå¯¦éŒ¯èª¤
for term, asr_output in zip(test_terms, asr_outputs):
    variants = generator.generate_variants(term)
    if asr_output not in variants:
        print(f"éºæ¼: {term} â†’ {asr_output}")
```

### æ€§èƒ½åŸºæº–æ¸¬è©¦
```python
import time

def benchmark_variant_generation():
    terms = ["å°åŒ—è»Šç«™", "æ°¸å’Œè±†æ¼¿", "å‹‡è€…é¬¥æƒ¡é¾"]
    generator = ChineseFuzzyGenerator()

    start = time.time()
    for term in terms * 100:
        variants = generator.generate_variants(term)
    elapsed = time.time() - start

    print(f"å¹³å‡æ¯è©: {elapsed/300*1000:.2f}ms")
    print(f"å¹³å‡è®Šé«”æ•¸: {sum(len(v) for v in variants)/len(terms):.1f}")
```

---

## ğŸ¯ ç¸½çµ

### æ ¸å¿ƒç™¼ç¾
1. âœ… **æ ¸å¿ƒç†å¿µæ­£ç¢º**ï¼šåœ¨æ‹¼éŸ³/éŸ³æ¨™ç¶­åº¦æ¯”å°ï¼Œè®Šé«”æ‹¼å¯«åªæ˜¯ UX
2. âœ… **Chinese/Japanese ç¬¦åˆç†å¿µ**ï¼šå¯¦ç¾æ­£ç¢ºï¼Œä½†æœ‰æ€§èƒ½å•é¡Œ
3. âŒ **English åé›¢ç†å¿µ**ï¼šæ²’æœ‰ IPA ç¶­åº¦ç”Ÿæˆï¼ˆP0 é—œéµå•é¡Œï¼‰
4. âš ï¸ **ç¼ºå°‘çµ±ä¸€æ¶æ§‹**ï¼šä¸‰å€‹æ¨¡çµ„è¨­è¨ˆä¸ä¸€è‡´

### é—œéµæ”¹é€²æ–¹å‘
1. **English æ¨¡çµ„é‡æ§‹** (P0) - å¯¦ç¾ IPA ç¶­åº¦è®Šé«”ç”Ÿæˆ
2. **Chinese æ€§èƒ½å„ªåŒ–** (P0) - åœ¨ç”Ÿæˆéšæ®µåŸºæ–¼æ‹¼éŸ³å»é‡
3. **Japanese å‹•æ…‹èª¿æ•´** (P0) - ç§»é™¤ä»»æ„ç¡¬ç·¨ç¢¼é™åˆ¶
4. **çµ±ä¸€æ¶æ§‹æŠ½è±¡** (P1) - BaseFuzzyGenerator åŸºé¡
5. **è®Šé«”è³ªé‡æå‡** (P1) - ç½®ä¿¡åº¦è©•åˆ†æ©Ÿåˆ¶
6. **Config å¯æ“´å±•æ€§** (P1) - å‹•æ…‹æ·»åŠ è¦å‰‡ API

### å¯¦æ–½å»ºè­°
- å„ªå…ˆä¿®å¾© P0 å•é¡Œ (1-2 é€±)
- é€æ­¥å®Œå–„ P1 æ”¹é€² (2-3 é€±)
- æŒçºŒå„ªåŒ– P2 å»ºè­° (é•·æœŸ)
- æ”¶é›†çœŸå¯¦ ASR/LLM éŒ¯èª¤æ•¸æ“šé©—è­‰æ•ˆæœ

---

**å ±å‘Šç”Ÿæˆæ™‚é–“**: 2025-12-07
**åˆ†æå°è±¡**: Phonofix v0.2.0
**åˆ†æè€…**: Claude Sonnet 4.5
