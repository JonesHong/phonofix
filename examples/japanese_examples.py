"""
æ—¥æ–‡èªéŸ³è¾¨è­˜æ ¡æ­£ç¯„ä¾‹ (Japanese ASR Correction Examples)

æœ¬æª”æ¡ˆå±•ç¤º UnifiedEngine (Japanese) çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŸºç¤ç”¨æ³• - è‡ªå‹•ç”Ÿæˆ Romaji ç´¢å¼•
2. æ‰‹å‹•åˆ¥å - æŒ‡å®šå¸¸è¦‹éŒ¯èª¤æ‹¼å¯«
3. ç™¼éŸ³è®Šé«” - é•·éŸ³ã€ä¿ƒéŸ³ã€åŠ©è©éŒ¯èª¤
4. ä¸Šä¸‹æ–‡é—œéµå­— - æ ¹æ“šå‰å¾Œæ–‡åˆ¤æ–·æ›¿æ› (åŒéŸ³ç•°ç¾©è©)
5. ä¸Šä¸‹æ–‡æ’é™¤ - é¿å…éŒ¯èª¤ä¿®æ­£
6. æ¬Šé‡ç³»çµ± - æ§åˆ¶æ›¿æ›å„ªå…ˆç´š
7. æ··åˆæ ¼å¼é…ç½®
8. é•·æ–‡ç« æ ¡æ­£
"""

import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent
sys.path.insert(0, str(root_dir))
sys.path.insert(0, str(root_dir / "src"))

from phonofix import UnifiedEngine
from tools.translation_client import translate_text

# Initialize Engine
engine = UnifiedEngine()
is_translate = False


def print_case(title, text, result, explanation):
    print(f"--- {title} ---")
    print(f"åŸæ–‡ (Original):  {text}")
    if is_translate:
        print(f"è­¯æ–‡ (Trans):     {translate_text(text)}")
    print(f"ä¿®æ­£ (Corrected): {result}")
    if is_translate:
        print(f"è­¯æ–‡ (Trans):     {translate_text(result)}")
    print(f"èªªæ˜ (Note):      {explanation}")
    print()


# =============================================================================
# ç¯„ä¾‹ 1: åŸºç¤ç”¨æ³• - è‡ªå‹•ç”Ÿæˆ Romaji ç´¢å¼•
# =============================================================================
def example_1_basic_usage():
    """
    æœ€ç°¡å–®çš„ç”¨æ³•ï¼šåªæä¾›æ­£ç¢ºè©å½™ï¼Œç³»çµ±è‡ªå‹•ç”Ÿæˆ Romaji ç´¢å¼•ã€‚
    The simplest usage: provide correct terms, system auto-generates Romaji index.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 1: åŸºç¤ç”¨æ³• (Basic Usage)")
    print("=" * 60)

    # åªéœ€æä¾›æ­£ç¢ºçš„è©å½™
    corrector = engine.create_corrector(
        [
            "ä¼šè­°",  # kaigi
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",  # purojekuto
            "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",  # enjinia
            "èƒƒã‚«ãƒ¡ãƒ©",  # ikamera
        ]
    )

    test_cases = [
        ("æ˜æ—¥ã®kaigiã«å‚åŠ ã—ã¾ã™", "Romaji -> Kanji (kaigi -> ä¼šè­°)"),
        (
            "æ–°ã—ã„purojekutoãŒå§‹ã¾ã‚Šã¾ã™",
            "Romaji -> Katakana (purojekuto -> ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ)",
        ),
        ("å½¼ã¯å„ªç§€ãªenjiniaã§ã™", "Romaji -> Katakana (enjinia -> ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢)"),
        ("ikameraã®æ¤œæŸ»", "Romaji -> Kanji/Katakana (ikamera -> èƒƒã‚«ãƒ¡ãƒ©)"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Basic", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 2: æ‰‹å‹•åˆ¥å (Manual Aliases)
# =============================================================================
def example_2_manual_aliases():
    """
    æ‰‹å‹•æä¾›åˆ¥åï¼Œè™•ç†ç‰¹æ®Šæ‹¼å¯«æˆ–ç°¡ç¨±ã€‚
    Manually provide aliases for special spellings or abbreviations.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 2: æ‰‹å‹•åˆ¥å (Manual Aliases)")
    print("=" * 60)

    corrector = engine.create_corrector(
        {
            "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³": ["sumaho", "smapho"],  # Abbreviation: ã‚¹ãƒãƒ›
            "ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿": ["pasokon"],  # Abbreviation: ãƒ‘ã‚½ã‚³ãƒ³
            "ã‚¢ã‚¹ãƒ”ãƒªãƒ³": ["asupirin", "asupirinn"],  # Common typo
        }
    )

    test_cases = [
        ("æ–°ã—ã„sumahoã‚’è²·ã„ã¾ã—ãŸ", "Abbreviation (sumaho -> ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³)"),
        ("pasokonãŒå£Šã‚Œã¾ã—ãŸ", "Abbreviation (pasokon -> ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿)"),
        ("é ­ç—›ã«asupirinn", "Typo correction (asupirinn -> ã‚¢ã‚¹ãƒ”ãƒªãƒ³)"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Manual Aliases", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 3: ç™¼éŸ³è®Šé«” (Phonetic Variants)
# =============================================================================
def example_3_phonetic_variants():
    """
    è™•ç†é•·éŸ³ã€ä¿ƒéŸ³éºæ¼æˆ–åŠ©è©éŒ¯èª¤ã€‚
    Handling missing long vowels, gemination, or particle errors.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 3: ç™¼éŸ³è®Šé«” (Phonetic Variants)")
    print("=" * 60)

    # term_map = {
    #     "é€šã‚Š": ["tori"],       # Missing long vowel (toori -> tori)
    #     "åˆ‡æ‰‹": ["kite"],       # Missing gemination (kitte -> kite)
    #     "ã“ã‚“ã«ã¡ã¯": ["konnichiwa"], # Particle wa/ha mismatch
    # }
    term_list = ["é€šã‚Š", "åˆ‡æ‰‹", "ã“ã‚“ã«ã¡ã¯"]
    corrector = engine.create_corrector(term_list)

    test_cases = [
        ("ã“ã®toriã¯è³‘ã‚„ã‹ã§ã™", "Long vowel correction (tori -> é€šã‚Š)"),
        ("kiteã‚’é›†ã‚ã¦ã„ã¾ã™", "Gemination correction (kite -> åˆ‡æ‰‹)"),
        ("å…ˆç”Ÿã€konnichiwa", "Particle correction (konnichiwa -> ã“ã‚“ã«ã¡ã¯)"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Variants", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 4: ä¸Šä¸‹æ–‡é—œéµå­— (Context Keywords)
# =============================================================================
def example_4_context_keywords():
    """
    ä½¿ç”¨ keywords é€²è¡ŒåŒéŸ³ç•°ç¾©è©è¾¨æ (Homophone Disambiguation)ã€‚
    Using keywords to disambiguate homophones.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 4: ä¸Šä¸‹æ–‡é—œéµå­— (Context Keywords)")
    print("=" * 60)

    corrector = engine.create_corrector(
        {
            "ç®¸": {
                "aliases": ["hashi"],
                "keywords": ["é£Ÿã¹ã‚‹", "ã”é£¯", "ä½¿ã†", "æŒã¤"],
                "weight": 0.5,
            },
            "æ©‹": {
                "aliases": ["hashi"],
                "keywords": ["æ¸¡ã‚‹", "å·", "é•·ã„", "å»ºè¨­"],
                "weight": 0.5,
            },
            "ç«¯": {
                "aliases": ["hashi"],
                "keywords": ["æ­©ã", "é“", "éš…"],
                "weight": 0.5,
            },
        }
    )

    test_cases = [
        ("hashiã‚’ä½¿ã£ã¦ã”é£¯ã‚’é£Ÿã¹ã‚‹", "Context: é£Ÿã¹ã‚‹ -> ç®¸ (Chopsticks)"),
        ("å·ã®hashiã‚’æ¸¡ã‚‹", "Context: æ¸¡ã‚‹/å· -> æ©‹ (Bridge)"),
        ("é“ã®hashiã‚’æ­©ã", "Context: æ­©ã/é“ -> ç«¯ (Edge)"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Keywords", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 5: ä¸Šä¸‹æ–‡æ’é™¤ (Context Exclusion)
# =============================================================================
def example_5_exclude_when():
    """
    ä½¿ç”¨ exclude_when é¿å…éŒ¯èª¤ä¿®æ­£ã€‚
    Using exclude_when to prevent incorrect corrections.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 5: ä¸Šä¸‹æ–‡æ’é™¤ (Context Exclusion)")
    print("=" * 60)

    corrector = engine.create_corrector(
        {
            "æ„›": {
                "aliases": ["ai"],
                "exclude_when": [
                    "äººå·¥çŸ¥èƒ½",
                    "ãƒ­ãƒœãƒƒãƒˆ",
                    "IT",
                ],  # Don't correct 'ai' to 'æ„›' in IT context
            }
        }
    )

    test_cases = [
        ("æ¯ã®aiã‚’æ„Ÿã˜ã‚‹", "No exclusion -> æ„› (Love)"),
        (
            "æœ€è¿‘ã®aiæŠ€è¡“ã¯ã™ã”ã„",
            "Excluded by 'æŠ€è¡“' (implied) or just 'ai' stays 'ai'? Wait, 'ai' matches 'æ„›'. If excluded, it stays 'ai'.",
        ),
        # Note: In our simple implementation, if excluded, it returns original token.
        ("ITä¼æ¥­ã®aié–‹ç™º", "Excluded by 'IT' -> ai (Artificial Intelligence)"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Exclusion", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 6: æ¬Šé‡ç³»çµ± (Weight System)
# =============================================================================
def example_6_weight_system():
    """
    ä½¿ç”¨æ¬Šé‡æ§åˆ¶å„ªå…ˆç´šã€‚
    Using weights to control priority.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 6: æ¬Šé‡ç³»çµ± (Weight System)")
    print("=" * 60)

    corrector = engine.create_corrector(
        {
            "æ©Ÿæ¢°": {"aliases": ["kikai"], "weight": 0.8},  # Higher priority (Machine)
            "æ©Ÿä¼š": {
                "aliases": ["kikai"],
                "weight": 0.2,  # Lower priority (Opportunity)
            },
        }
    )

    test_cases = [
        ("æ–°ã—ã„kikaiã‚’å°å…¥ã™ã‚‹", "High weight -> æ©Ÿæ¢° (Machine)"),
        # Note: Without keywords, weight decides.
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Weight", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 7: ç™ºéŸ³å¤‰ä½“å±•ç¤º (Phonetic Variants)
# =============================================================================
def example_7_phonetic_variants():
    """
    å±•ç¤º JapaneseFuzzyGenerator ç”Ÿæˆçš„ç™¼éŸ³è®Šé«”ã€‚
    Show generated phonetic variants for given terms.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 7: ç™ºéŸ³å¤‰ä½“å±•ç¤º (Phonetic Variants)")
    print("=" * 60)

    from phonofix.languages.japanese.fuzzy_generator import JapaneseFuzzyGenerator

    generator = JapaneseFuzzyGenerator()

    terms = [
        "é€šã‚Š",
        "åˆ‡æ‰‹",
        "ã“ã‚“ã«ã¡ã¯",
        "æ±äº¬",
        "å¤§é˜ª",
        "äº¬éƒ½",
        "ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³",
    ]

    for term in terms:
        variants = generator.generate_variants(term)
        print(f"ç›®æ¨™è©: {term}")
        print(f"ç”Ÿæˆçš„è®Šé«”æ•¸: {len(variants)}")
        print(f"å‰10å€‹è®Šé«”: {variants[:10]}")
        print(f"èªªæ˜: å±•ç¤ºè‡ªå‹•ç”Ÿæˆçš„ ASR èª¤è½æ‹¼å¯«è®Šé«”")
        print()


# =============================================================================
# ç¯„ä¾‹ 8: æ··åˆæ ¼å¼ (Mixed Format)
# =============================================================================
def example_8_mixed_format():
    """
    æ··åˆä½¿ç”¨åˆ—è¡¨å’Œå­—å…¸é…ç½®ã€‚
    Mixing list and dictionary configurations.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 8: æ··åˆæ ¼å¼ (Mixed Format)")
    print("=" * 60)

    corrector = engine.create_corrector(
        {
            "æ±äº¬": ["tokyo"],  # Simple list
            "å¤§é˜ª": {},  # Empty dict (Auto-generate)
            "äº¬éƒ½": {  # Full config
                "aliases": ["kyoto"],
                "keywords": ["å¯º", "è¦³å…‰"],
                "weight": 0.5,
            },
        }
    )

    test_cases = [
        ("tokyoã«è¡ŒããŸã„", "Simple list -> æ±äº¬"),
        ("osakaã®ãŸã“ç„¼ã", "Auto-gen -> å¤§é˜ª"),
        ("kyo toã®å¯ºã‚’è¦‹å­¦", "Full config -> äº¬éƒ½"),
    ]

    for text, explanation in test_cases:
        result = corrector.correct(text)
        print_case("Mixed", text, result, explanation)


# =============================================================================
# ç¯„ä¾‹ 8: é•·æ–‡ç« æ ¡æ­£ (Long Article)
# =============================================================================
def example_8_long_article():
    """
    é•·æ–‡ç« ç¶œåˆæ¸¬è©¦ã€‚
    Comprehensive test with a longer article.
    """
    print("=" * 60)
    print("ç¯„ä¾‹ 8: é•·æ–‡ç« æ ¡æ­£ (Long Article)")
    print("=" * 60)

    terms = {
        "äººå·¥çŸ¥èƒ½": ["ai"],
        "é–‹ç™º": ["kaihatsu"],
        "æœªæ¥": ["mirai"],
        "æŠ€è¡“": ["gijutsu"],
        "ç¤¾ä¼š": ["shakai"],
        "å¤‰é©": ["henkaku"],
        "ãƒ­ãƒœãƒƒãƒˆ": ["robotto"],
    }

    corrector = engine.create_corrector(terms)

    article = (
        "ç¾åœ¨ã€aiã®gijutsuã¯æ€¥é€Ÿã«é€²æ­©ã—ã¦ã„ã¾ã™ã€‚"
        "å¤šãã®ä¼æ¥­ãŒæ–°ã—ã„robottoã®kaihatsuã«å–ã‚Šçµ„ã‚“ã§ãŠã‚Šã€"
        "ã“ã‚ŒãŒç§ãŸã¡ã®shakaiã«å¤§ããªhenkakuã‚’ã‚‚ãŸã‚‰ã™ã§ã—ã‚‡ã†ã€‚"
        "æ˜ã‚‹ã„miraiã®ãŸã‚ã«ã€ç§ãŸã¡ã¯å­¦ã³ç¶šã‘ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
    )

    print("åŸæ–‡ (Original):")
    print(article)
    print(f"è­¯æ–‡: {translate_text(article)}")
    print("-" * 40)

    result = corrector.correct(article)

    print("ä¿®æ­£å¾Œ (Corrected):")
    print(result)
    print(f"è­¯æ–‡: {translate_text(result)}")
    print("-" * 40)


# =============================================================================
# ä¸»ç¨‹å¼
# =============================================================================
if __name__ == "__main__":
    print("\n" + "ğŸ‡¯ğŸ‡µ" * 20)
    print("  æ—¥æ–‡èªéŸ³è¾¨è­˜æ ¡æ­£ç¯„ä¾‹ (Japanese Examples)")
    print("ğŸ‡¯ğŸ‡µ" * 20 + "\n")

    examples = [
        example_1_basic_usage,
        example_2_manual_aliases,
        example_3_phonetic_variants,
        example_4_context_keywords,
        example_5_exclude_when,
        example_6_weight_system,
        example_7_phonetic_variants,
        example_8_mixed_format,
        example_8_long_article,
    ]

    for func in examples:
        try:
            func()
        except Exception as e:
            print(f"âŒ ç¯„ä¾‹åŸ·è¡Œå¤±æ•—: {e}")
            import traceback

            traceback.print_exc()
        print()

    print("=" * 60)
    print("âœ… æ‰€æœ‰ç¯„ä¾‹åŸ·è¡Œå®Œæˆ!")
    print("=" * 60)
