**English** | [ç¹é«”ä¸­æ–‡](README.zh-TW.md)

# Phonetic Substitution Engine

A multi-language phonetic similarity-based proper noun substitution tool. Supports ASR/LLM post-processing, regional vocabulary conversion, abbreviation expansion, and various other use cases.
Supports **Code-Switching** (mixed Chinese-English) scenarios via manual chaining of correctors.

## ğŸ’¡ Core Philosophy

**This package does not maintain any proper noun dictionaries; instead, it provides a substitution engine based on phonetic vector space.**

The core mechanism of this package is to uniformly map text from different languages into a **phonetic vector space** (composed of Pinyin and IPA phonetic symbols).

Spelling errors can come from ASR, LLMs, or manual typing. Instead of overfitting to any single source, this tool converts text into the **Pinyin/IPA dimension**, then uses **fuzzy phonetic variants** to map misspellings back to the user-defined canonical proper nouns.

> âš ï¸ **Note**: This is not a full-text spell checker, but focuses on "phonetic similarity substitution for proper nouns."

Users must provide their own proper noun dictionary. This tool will:
1. **Automatically generate phonetic variants**:
   - **Chinese**: Automatically generate Taiwanese accent/fuzzy phonetic variants (e.g., "åŒ—è»Š" â†’ "å°åŒ—è»Šç«™")
   - **English**: Calculate phonetic similarity based on IPA (International Phonetic Alphabet) (e.g., "Ten so floor" â†’ "TensorFlow")
2. **Intelligent vocabulary substitution**: Automatically identify language segments and replace phonetically similar words with your specified standard proper nouns

**Use Cases**:
- **Spelling-Error Post-Processing**: Restore proper nouns in noisy text (ASR/LLM/manual typing; including mixed Chinese-English)
- **LLM Output Post-Processing**: Correct homophone/near-homophone errors when LLMs choose wrong characters for rare proper nouns
- **Proper Noun Standardization**: Restore colloquial/misspelled terms to their formal names
- **Regional Vocabulary Conversion**: Mainland China terms â†” Taiwan terms

## ğŸ“š Features

### 1. Multi-Language Support
- **Language Engines**: Use `ChineseEngine` / `EnglishEngine` / `JapaneseEngine` per language (for mixed-language inputs, chain multiple correctors manually)
- **English Phonetic Substitution**: 
    - Uses IPA (International Phonetic Alphabet) for phonetic similarity matching
    - Supports phonetic restoration of acronyms
- **Chinese Phonetic Substitution**:
    - Uses Pinyin for fuzzy phonetic matching
    - Supports Taiwanese Mandarin-specific pronunciation confusion patterns
- **Japanese Phonetic Substitution**:
    - Uses Romaji (Hepburn) for phonetic similarity matching
    - Supports context-aware reading generation (e.g., particle "ha" -> "wa")

### 2. Automatic Phonetic Variant Generation
- **Chinese**: Automatically generates Taiwanese accent/fuzzy phonetic variants
  - Retroflex consonants (z/zh, c/ch, s/sh)
  - n/l confusion (Taiwanese Mandarin)
  - r/l confusion, f/h confusion
  - Final vowel confusion (in/ing, en/eng, ue/ie, etc.)
- **English**: Automatically generates common phonetic-variant spellings (ASR/LLM/typing)
  - Syllable split variants ("TensorFlow" â†’ "Ten so floor")
  - Acronym expansion variants ("AWS" â†’ "A W S")

### 3. Intelligent Substitution Engine
- Sliding window matching algorithm
- Context keyword weighting mechanism
- Dynamic tolerance rate adjustment

### 4. Streaming (Removed)

Streaming APIs (`StreamingCorrector`, `ChunkStreamingCorrector`) were removed in `v0.2.0` during the language-module refactor.

## ğŸ“¦ Installation

### Using uv (Recommended)

[uv](https://docs.astral.sh/uv/) is the next-generation Python package manager, fast and feature-complete.

```bash
# Install uv (Windows PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Install uv (macOS/Linux)
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# Default installation (includes both Chinese and English support)
uv add phonofix

# Chinese support only
uv add "phonofix[ch]"

# English support only (requires espeak-ng installation, see below)
uv add "phonofix[en]"

# Full installation (same as default)
uv add "phonofix[all]"
```

### English Support (espeak-ng Installation)

English phonetic features depend on the [espeak-ng](https://github.com/espeak-ng/espeak-ng) system package.

**Using Built-in Installation Scripts (Recommended)**:

This project provides automated installation scripts that will download, install, and configure environment variables automatically:

```bash
# Windows PowerShell (recommended to run as Administrator)
.\scripts\setup_espeak.ps1

# Windows CMD (recommended to run as Administrator)
scripts\setup_espeak_windows.bat

# macOS / Linux
chmod +x scripts/setup_espeak.sh
./scripts/setup_espeak.sh
```

The scripts will automatically:
1. Check/install espeak-ng
2. Set the `PHONEMIZER_ESPEAK_LIBRARY` environment variable
3. Verify that phonemizer works correctly

**Manual Installation**:

```bash
# Windows: Download installer from GitHub
# https://github.com/espeak-ng/espeak-ng/releases

# macOS
brew install espeak-ng

# Debian/Ubuntu
sudo apt install espeak-ng

# Arch Linux
sudo pacman -S espeak-ng
```

### Japanese Support (Optional)

Japanese support requires `cutlet`, `fugashi`, and `unidic-lite`.

```bash
pip install "phonofix[ja]"
```

### Development Environment Setup

```bash
# Install dependencies after cloning
uv sync

# Install dev dependencies
uv sync --dev

# Run examples
uv run ./examples/chinese_examples.py

# Run tests
uv run pytest
```

## ğŸ§ª Development

```bash
# Run tests
uv run pytest

# Run tests with coverage
uv run pytest --cov

# Code formatting
uv run ruff format .

# Code linting
uv run ruff check .

# Type checking
uv run mypy src/phonofix
```

## ğŸš€ Quick Start

### 1. Mixed Language Substitution (Manual Pipeline)

```python
from phonofix import ChineseEngine, EnglishEngine

ch_engine = ChineseEngine()
en_engine = EnglishEngine()

ch_corrector = ch_engine.create_corrector(["å°åŒ—è»Šç«™", "ç‰›å¥¶", "ç„¶å¾Œ"])
en_corrector = en_engine.create_corrector({
    "TensorFlow": ["Ten so floor"],
    "Python": ["Pyton"],
})

def correct(text: str) -> str:
    text = en_corrector.correct(text, full_context=text)
    text = ch_corrector.correct(text, full_context=text)
    return text

# Text post-processing (ASR/LLM/manual typos)
noisy_text = "æˆ‘åœ¨èƒåŒ—è»Šç«™ç”¨Pytonå¯«Ten so floorçš„code"
print(correct(noisy_text))
# Output: "æˆ‘åœ¨å°åŒ—è»Šç«™ç”¨Pythonå¯«TensorFlowçš„code"

# LLM output post-processing (LLM may choose wrong homophones for rare words)
llm_text = "æˆ‘åœ¨èƒåŒ—è»Šç«™ç”¨æ´¾æ£®å¯«code"  # LLM transliterated Python as "æ´¾æ£®"
print(correct(llm_text))
# Output: "æˆ‘åœ¨å°åŒ—è»Šç«™ç”¨Pythonå¯«code"
```

### 2. Japanese Support

Japanese support uses Romaji (Hepburn) for phonetic matching.

```python
from phonofix import JapaneseEngine

engine = JapaneseEngine()
corrector = engine.create_corrector({
    "ã‚¢ã‚¹ãƒ”ãƒªãƒ³": ["asupirin"],  # Aspirin
    "ãƒ­ã‚­ã‚½ãƒ‹ãƒ³": ["rokisonin"], # Loxonin
    "èƒƒã‚«ãƒ¡ãƒ©": ["ikamera"]      # Gastrocamera
})

text = "é ­ãŒç—›ã„ã®ã§asupirinã‚’é£²ã¿ã¾ã—ãŸ"
result = corrector.correct(text)
print(result)
# Output: "é ­ãŒç—›ã„ã®ã§ã‚¢ã‚¹ãƒ”ãƒªãƒ³ã‚’é£²ã¿ã¾ã—ãŸ"
```

### 3. Chinese Only (Chinese Engine)

**Important Note**: This tool does not provide a default dictionary. You need to create your own proper noun list based on your business scenario.

#### Recommended Usage - Auto-generate Aliases

Using `ChineseEngine`, **you only need to provide your proper noun list**, and the tool will automatically generate all possible fuzzy phonetic variants with Pinyin deduplication:

#### Simplest Format - Keyword List Only

```python
from phonofix import ChineseEngine

# Step 1: Provide your proper noun list (this is the dictionary you need to maintain)
my_terms = ["å°åŒ—è»Šç«™", "ç‰›å¥¶", "ç™¼æ®"]

# Step 2: Initialize engine and create corrector
# The tool will automatically generate all possible fuzzy phonetic variants
# For example: "å°åŒ—è»Šç«™" â†’ automatically generates phonetically similar variants (e.g., "èƒåŒ—è»Šç«™")
engine = ChineseEngine()
corrector = engine.create_corrector(my_terms)

# Step 3: Automatically convert phonetically similar words to correct proper nouns
result = corrector.correct("æˆ‘åœ¨èƒåŒ—è»Šç«™è²·äº†æµå¥¶,ä»–èŠ±æ®äº†æ‰èƒ½")
# Result: 'æˆ‘åœ¨å°åŒ—è»Šç«™è²·äº†ç‰›å¥¶,ä»–ç™¼æ®äº†æ‰èƒ½'
# Explanation: "èƒåŒ—è»Šç«™" â†’ "å°åŒ—è»Šç«™", "æµå¥¶" â†’ "ç‰›å¥¶", "èŠ±æ®" â†’ "ç™¼æ®"
```

#### Full Format - Aliases + Keywords + Weights

When the same alias may correspond to multiple proper nouns, use context keywords and weights to improve accuracy:

```python
# Your proper noun dictionary (maintain based on your business scenario)
my_business_terms = {
    "æ°¸å’Œè±†æ¼¿": {
        "aliases": ["æ°¸è±†", "å‹‡è±†"],  # Manually provide common aliases or mispronunciations
        "keywords": ["åƒ", "å–", "è²·", "å®µå¤œ"],  # Context keywords help with judgment
        "weight": 0.3  # Matching weight
    },
    "å‹‡è€…é¬¥æƒ¡é¾": {
        "aliases": ["å‹‡é¬¥", "æ°¸é¬¥"],  # Homophones with different meanings
        "keywords": ["ç©", "éŠæˆ²", "æ”»ç•¥"],
        "weight": 0.2
    }
}

engine = ChineseEngine()
corrector = engine.create_corrector(my_business_terms)

result = corrector.correct("æˆ‘å»è²·å‹‡é¬¥ç•¶å®µå¤œ")
# Result: 'æˆ‘å»è²·æ°¸å’Œè±†æ¼¿ç•¶å®µå¤œ'
# Explanation: Matched "è²·" and "å®µå¤œ" keywords, determined to be "æ°¸å’Œè±†æ¼¿" instead of "å‹‡è€…é¬¥æƒ¡é¾"
```

**Advantages**:
- âœ… Automatically generates fuzzy phonetic variants, no manual maintenance required
- âœ… Automatically filters Pinyin-duplicate aliases (similar to Set behavior)
- âœ… Supports multiple input formats, flexible usage
- âœ… Reduces configuration effort, focus on core vocabulary

### Advanced Features

#### Context Keywords

```python
from phonofix import ChineseEngine

# Use context keywords to improve accuracy
engine = ChineseEngine()
corrector = engine.create_corrector({
    "æ°¸å’Œè±†æ¼¿": {
        "aliases": ["æ°¸è±†"],
        "keywords": ["åƒ", "å–", "è²·", "å®µå¤œ", "æ—©é¤"]
    },
    "å‹‡è€…é¬¥æƒ¡é¾": {
        "aliases": ["å‹‡é¬¥"],
        "keywords": ["ç©", "éŠæˆ²", "é›»å‹•", "æ”»ç•¥"]
    }
})

result = corrector.correct("æˆ‘å»è²·å‹‡é¬¥ç•¶å®µå¤œ")  # Matched "è²·" â†’ æ°¸å’Œè±†æ¼¿
result = corrector.correct("é€™æ¬¾æ°¸è±†çš„æ”»ç•¥å¾ˆé›£æ‰¾")  # Matched "æ”»ç•¥" â†’ å‹‡è€…é¬¥æƒ¡é¾
```

#### Weight System

```python
# Use weights to increase priority
engine = ChineseEngine()
corrector = engine.create_corrector({
    "æ©å…¸": {
        "aliases": ["å®‰é»"],
        "weight": 0.3  # High weight, priority matching
    },
    "ä¸Šå¸": {
        "aliases": ["ä¸Šå¸"],
        "weight": 0.1  # Low weight
    }
})
```

#### Protected Terms

```python
# Set protected terms list to prevent specific words from being corrected
engine = ChineseEngine()
corrector = engine.create_corrector(
    terms={
        "å°åŒ—è»Šç«™": ["åŒ—è»Š"]
    },
    protected_terms=["åŒ—å´", "å—å´"]  # These words will not be corrected
)

result = corrector.correct("æˆ‘åœ¨åŒ—å´ç­‰ä½ ")  # Will not be corrected to "å°åŒ—è»Šç«™å´"
```

### 4. English Only (English Engine)

```python
from phonofix import EnglishEngine

engine = EnglishEngine()
corrector = engine.create_corrector({
    "TensorFlow": ["Ten so floor"],
    "Python": ["Pyton"]
})

result = corrector.correct("I use Pyton to write Ten so floor code")
# Output: "I use Python to write TensorFlow code"
```

### 5. Incremental Inputs (No Streaming API)

Streaming APIs were removed in `v0.2.0`. For incremental ASR/LLM inputs, a simple and stable approach is: accumulate the full text and re-run `correct()` on each update.

## ğŸ“ Project Structure

```
phonofix/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ phonofix/                      # Main package (src layout)
â”‚       â”œâ”€â”€ __init__.py                # Main entry, exports ChineseEngine/EnglishEngine/JapaneseEngine, etc.
â”‚       â”‚
â”‚       â”œâ”€â”€ core/                      # Cross-module shared contracts/utilities
â”‚       â”‚   â”œâ”€â”€ protocols/             # Protocols (Corrector/Fuzzy)
â”‚       â”‚   â”œâ”€â”€ term_config.py         # Term dict normalization
â”‚       â”‚   â””â”€â”€ engine_interface.py    # CorrectorEngine abstract base
â”‚       â”‚
â”‚       â”œâ”€â”€ backend/                   # Phonetic backend (phonemizer/pypinyin wrapper)
â”‚       â”‚   â”œâ”€â”€ base.py                # PhoneticBackend abstract class
â”‚       â”‚   â”œâ”€â”€ chinese_backend.py     # Chinese Pinyin backend
â”‚       â”‚   â””â”€â”€ english_backend.py     # English IPA backend
â”‚       â”‚
â”‚       â”œâ”€â”€ languages/                 # Language-specific implementations
â”‚       â”‚   â”œâ”€â”€ chinese/               # Chinese module
â”‚       â”‚   â”‚   â”œâ”€â”€ config.py          # Pinyin config (initials/finals/fuzzy sounds)
â”‚       â”‚   â”‚   â”œâ”€â”€ corrector.py       # Chinese corrector
â”‚       â”‚   â”‚   â”œâ”€â”€ engine.py          # ChineseEngine
â”‚       â”‚   â”‚   â”œâ”€â”€ fuzzy_generator.py # Fuzzy phonetic variant generator
â”‚       â”‚   â”‚   â”œâ”€â”€ number_variants.py # Number variant handling
â”‚       â”‚   â”‚   â””â”€â”€ tokenizer.py       # Chinese tokenizer
â”‚       â”‚   â”‚
â”‚       â”‚   â”œâ”€â”€ english/               # English module
â”‚       â”‚   â”‚   â”œâ”€â”€ config.py          # IPA config
â”‚       â”‚   â”‚   â”œâ”€â”€ corrector.py       # English corrector
â”‚       â”‚   â”‚   â”œâ”€â”€ engine.py          # EnglishEngine
â”‚       â”‚   â”‚   â”œâ”€â”€ fuzzy_generator.py # Syllable split variant generator
â”‚       â”‚   â”‚   â””â”€â”€ tokenizer.py       # English tokenizer
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ japanese/              # Japanese module
â”‚       â”‚       â”œâ”€â”€ config.py
â”‚       â”‚       â”œâ”€â”€ corrector.py
â”‚       â”‚       â”œâ”€â”€ engine.py          # JapaneseEngine
â”‚       â”‚       â”œâ”€â”€ fuzzy_generator.py
â”‚       â”‚       â””â”€â”€ tokenizer.py
â”‚       â”‚
â”‚       â””â”€â”€ utils/                     # Utility modules
â”‚           â”œâ”€â”€ lazy_imports.py        # Lazy imports (optional dependency management)
â”‚           â””â”€â”€ logger.py              # Logging utilities
â”‚
â”œâ”€â”€ scripts/                           # Installation scripts
â”‚   â”œâ”€â”€ setup_espeak.ps1               # Windows PowerShell espeak-ng installer
â”‚   â”œâ”€â”€ setup_espeak.sh                # macOS/Linux espeak-ng installer
â”‚   â””â”€â”€ setup_espeak_windows.bat       # Windows CMD espeak-ng installer
â”‚
â”œâ”€â”€ examples/                          # Usage examples
â”‚   â”œâ”€â”€ chinese_examples.py            # Chinese correction examples
â”‚   â”œâ”€â”€ english_examples.py            # English correction examples
â”‚   â”œâ”€â”€ japanese_examples.py           # Japanese correction examples
â”‚   â”œâ”€â”€ _example_utils.py              # Shared helpers (CLI/output/translation)
â”‚   â””â”€â”€ README.md                      # Example design notes
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â”œâ”€â”€ test_chinese_corrector.py
â”‚   â”œâ”€â”€ test_english_corrector.py
â”‚   â”œâ”€â”€ test_japanese_corrector.py
â”‚   â””â”€â”€ test_language_contracts.py
â”‚
â”œâ”€â”€ pyproject.toml                     # Project configuration (phonofix)
â”œâ”€â”€ requirements.txt                   # Dependency list
â””â”€â”€ README.md
```

## ğŸ¯ Use Cases

The following examples demonstrate how to create your own proper noun dictionary for different business scenarios:

### 1. Text Post-Processing

**Problem**: Speech recognition often mishears proper nouns as phonetically similar common words

```python
from phonofix import ChineseEngine, EnglishEngine

ch_corrector = ChineseEngine().create_corrector(["ç‰›å¥¶", "ç™¼æ®", "ç„¶å¾Œ"])
en_corrector = EnglishEngine().create_corrector({
    "TensorFlow": ["Ten so floor"],
    "Kubernetes": ["koo ber netes"],
})

# Noisy input: proper nouns misheard/misspelled
noisy_input = "æˆ‘è²·äº†æµå¥¶ï¼Œè˜­å¾Œç”¨Ten so floorè¨“ç·´æ¨¡å‹"
result = ch_corrector.correct(en_corrector.correct(noisy_input, full_context=noisy_input), full_context=noisy_input)
# Result: "æˆ‘è²·äº†ç‰›å¥¶ï¼Œç„¶å¾Œç”¨TensorFlowè¨“ç·´æ¨¡å‹"
```

### 2. LLM Output Post-Processing

**Problem**: LLMs may choose phonetically similar common characters for rare proper nouns

```python
from phonofix import ChineseEngine, EnglishEngine

ch_corrector = ChineseEngine().create_corrector(["è€¶ç©Œ", "æ©å…¸"])
en_corrector = EnglishEngine().create_corrector({
    "PyTorch": ["pie torch"],
    "NumPy": ["num pie"],
})

# LLM output: rare proper nouns replaced with homophone common characters
llm_output = "è€¶ç©Œçš„æ©é»å¾ˆå¤§ï¼Œæˆ‘ç”¨æ’ç‚¬å’Œå—æ´¾åšæ©Ÿå™¨å­¸ç¿’"
result = ch_corrector.correct(en_corrector.correct(llm_output, full_context=llm_output), full_context=llm_output)
# Result: "è€¶ç©Œçš„æ©å…¸å¾ˆå¤§ï¼Œæˆ‘ç”¨PyTorchå’ŒNumPyåšæ©Ÿå™¨å­¸ç¿’"
```

### 3. Regional Vocabulary Conversion

**Your Dictionary**: Maintain regional mapping table (e.g., Mainland China â†” Taiwan terms)

```python
# Your regional vocabulary dictionary
region_terms = {
    "é¦¬éˆ´è–¯": {"aliases": ["åœŸè±†"], "weight": 0.0},
    "å½±ç‰‡": {"aliases": ["è¦–é »"], "weight": 0.0}
}

engine = ChineseEngine()
corrector = engine.create_corrector(region_terms)

result = corrector.correct("æˆ‘ç”¨åœŸè±†åšäº†è¦–é »")
# Result: "æˆ‘ç”¨é¦¬éˆ´è–¯åšäº†å½±ç‰‡"
```

### 4. Abbreviation Expansion

**Your Dictionary**: Maintain common abbreviations and full names mapping

```python
# Your abbreviation dictionary
abbreviation_terms = {
    "å°åŒ—è»Šç«™": {"aliases": ["åŒ—è»Š"], "weight": 0.0}
}

engine = ChineseEngine()
corrector = engine.create_corrector(abbreviation_terms)

result = corrector.correct("æˆ‘åœ¨åŒ—è»Šç­‰ä½ ")
# Result: "æˆ‘åœ¨å°åŒ—è»Šç«™ç­‰ä½ "
```

### 5. Professional Terminology Standardization

**Your Dictionary**: Maintain professional terminology for your business domain

```python
# Your medical terminology dictionary
medical_terms = {
    "é˜¿æ–¯åŒ¹éˆ": {"aliases": ["é˜¿æ–¯åŒ¹æ—", "äºŒå››æ‰¹æ—"], "weight": 0.2}
}

engine = ChineseEngine()
corrector = engine.create_corrector(medical_terms)

result = corrector.correct("é†«ç”Ÿé–‹äº†äºŒå››æ‰¹æ—çµ¦æˆ‘")
# Result: "é†«ç”Ÿé–‹äº†é˜¿æ–¯åŒ¹éˆçµ¦æˆ‘"
```

## ğŸ“– Complete Examples

Please refer to the `examples/` directory, which contains multiple usage examples:

| File | Description |
|------|-------------|
| `chinese_examples.py` | Chinese phonetic substitution examples |
| `english_examples.py` | English phonetic substitution examples |
| `japanese_examples.py` | Japanese phonetic substitution examples |

```bash
# Run Chinese examples
uv run ./examples/chinese_examples.py

# Run English examples (requires espeak-ng)
uv run ./examples/english_examples.py

# Run Japanese examples (requires cutlet/fugashi/unidic-lite)
uv run ./examples/japanese_examples.py
```

## ğŸ”§ Technical Details

### Phonetic Matching Mechanism

#### Chinese: Pinyin Fuzzy Sound Rules

**Initial Consonant Fuzzy Groups**
| Group | Phonemes | Description |
|-------|----------|-------------|
| Retroflex | z â‡„ zh, c â‡„ ch, s â‡„ sh | Common in Taiwanese Mandarin |
| n/l confusion | n â‡„ l | Taiwanese Mandarin characteristic |
| r/l confusion | r â‡„ l | Common in ASR/typing |
| f/h confusion | f â‡„ h | Dialect influence |

**Final Vowel Fuzzy Mapping**
- `in` â‡„ `ing`, `en` â‡„ `eng`, `an` â‡„ `ang`
- `ian` â‡„ `iang`, `uan` â‡„ `uang`, `uan` â‡„ `an`
- `ong` â‡„ `eng`, `uo` â‡„ `o`, `ue` â‡„ `ie`

**Special Syllable Mappings**
- `fa` â‡„ `hua` (ç™¼/èŠ±)
- `xue` â‡„ `xie` (å­¸/é‹)
- `ran` â‡„ `lan`, `yan` (ç„¶/è˜­/åš´)
- For more, please refer to `src/phonofix/languages/chinese/config.py`

#### English: IPA Phonetic Matching

Uses [phonemizer](https://github.com/bootphon/phonemizer) to convert English to IPA (International Phonetic Alphabet), then calculates Levenshtein edit distance.

**Common Error Patterns**
| Error Type | Example | Description |
|------------|---------|-------------|
| Syllable splitting | "TensorFlow" â†’ "Ten so floor" | Speech recognition split error |
| Homophone | "Python" â†’ "Pyton" | Spelling error |
| Acronym expansion | "API" â†’ "A P I" | Letter-by-letter pronunciation |

### Keywords and exclude_when Mechanism

When the same alias may correspond to multiple proper nouns, use `keywords` and `exclude_when` for precise judgment:

```
Substitution Logic:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input text contains alias (e.g., "1kg")                â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Step 1: Check exclude_when (exclusion conditions)â”‚    â”‚
â”‚  â”‚   - If text contains any exclusion word â†’ No sub âŒâ”‚   â”‚
â”‚  â”‚   - e.g.: "1kgæ°´å¾ˆé‡" contains "æ°´" â†’ No sub to EKGâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“ (No exclusion match)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Step 2: Check keywords (required conditions)     â”‚    â”‚
â”‚  â”‚   - If keywords set and none match â†’ No sub âŒ    â”‚    â”‚
â”‚  â”‚   - If keywords set and matched â†’ Substitute âœ…   â”‚    â”‚
â”‚  â”‚   - If no keywords set â†’ Substitute âœ…            â”‚    â”‚
â”‚  â”‚   - e.g.: "1kgè¨­å‚™" contains "è¨­å‚™" â†’ Sub to EKG  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Important Rule: exclude_when Takes Priority Over Keywords**

Even if keywords match, no substitution occurs if exclude_when matches:

```python
"EKG": {
    "aliases": ["1kg"],
    "keywords": ["è¨­å‚™", "é†«ç™‚"],      # Must contain one to substitute
    "exclude_when": ["é‡", "å…¬æ–¤"],    # Contains any = no substitution
}

# Examples:
"é€™å€‹è¨­å‚™æœ‰ 1kgé‡"  # keywords(è¨­å‚™) âœ“ + exclude_when(é‡) âœ“ â†’ No substitution
"é€™å€‹ 1kgè¨­å‚™"      # keywords(è¨­å‚™) âœ“ + exclude_when âœ— â†’ Substitute to EKG
"è²·äº† 1kgçš„æ±è¥¿"    # keywords âœ— â†’ No substitution
```

### Substitution Algorithm Flow

```
Input Text
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Build Protection Mask             â”‚
â”‚    Mark positions of protected_terms â”‚
â”‚    These positions skip substitution â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Choose Corrector(s)               â”‚
â”‚    Per language: one corrector       â”‚
â”‚    Mixed: chain multiple correctors  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Sliding Window Scan               â”‚
â”‚    Traverse all possible word length â”‚
â”‚    combinations                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Phonetic Similarity Calculation   â”‚
â”‚    Chinese: Pinyin (specialâ†’finalâ†’   â”‚
â”‚             edit distance)           â”‚
â”‚    English: IPA edit distance        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Keywords/exclude_when Filtering   â”‚
â”‚    - exclude_when matched â†’ Skip     â”‚
â”‚    - No keywords matched â†’ Skip      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Calculate Final Score             â”‚
â”‚    Score = error_rate - weight -     â”‚
â”‚            context_bonus             â”‚
â”‚    (Lower score is better)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Conflict Resolution               â”‚
â”‚    Sort by score, select best        â”‚
â”‚    non-overlapping candidates        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Text Replacement                  â”‚
â”‚    Replace from back to front to     â”‚
â”‚    avoid index shifting              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output Result
```

### Dynamic Tolerance Rate

| Word Length | Tolerance | Description |
|-------------|-----------|-------------|
| 2 chars/letters | 0.20 | Must be very accurate |
| 3 chars/letters | 0.30 | Moderately strict |
| 4+ chars/letters | 0.40 | Higher tolerance |
| English mixed | 0.45 | Higher tolerance |

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

## ğŸ“„ License

MIT License

## ğŸ‘¨â€ğŸ’» Author

JonesHong

## ğŸ™ Acknowledgments

Thanks to the following projects:
- [pypinyin](https://github.com/mozillazg/python-pinyin)
- [python-Levenshtein](https://github.com/maxbachmann/Levenshtein)
- [Pinyin2Hanzi](https://github.com/letiantian/Pinyin2Hanzi)
- [hanziconv](https://github.com/berniey/hanziconv)
